{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "017527c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2731ef3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageClassification(\n",
      "    crop_size=[224]\n",
      "    resize_size=[256]\n",
      "    mean=[0.485, 0.456, 0.406]\n",
      "    std=[0.229, 0.224, 0.225]\n",
      "    interpolation=InterpolationMode.BILINEAR\n",
      ")\n",
      "10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\timfl\\anaconda3\\envs\\IML\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\timfl\\anaconda3\\envs\\IML\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nmodel_1 = torchvision.models.resnet18(pretrained = True)\\nprint(model._modules)\\nprint('----------------')\\nprint(model_1._modules)\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "# Initialize model\n",
    "resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "\n",
    "train_transforms = transforms.Compose([transforms.ToTensor()])\n",
    "weights = ResNet18_Weights.DEFAULT\n",
    "print(weights.transforms())\n",
    "\n",
    "train_transforms = transforms.Compose([transforms.ToTensor(), weights.transforms()])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root=\"dataset/\", transform=train_transforms)\n",
    "print(len(train_dataset))\n",
    "\n",
    "model = torchvision.models.resnet18(pretrained = True)\n",
    "print(model)\n",
    "\"\"\"\n",
    "model_1 = torchvision.models.resnet18(pretrained = True)\n",
    "print(model._modules)\n",
    "print('----------------')\n",
    "print(model_1._modules)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b9ecd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=16,\n",
    "                          shuffle=False,\n",
    "                          pin_memory=True, num_workers=4,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9aca2232",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = model._modules.get('avgpool')\n",
    "model.fc = torch.nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad379f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display image and label.\n",
    "#i = iter(train_loader)\n",
    "#features,_ = next(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "912057ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_embedding = model(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c438819a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=16\n",
    "samples=10000\n",
    "features=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "92604cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_mat = np.zeros((samples, features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dd7fced1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(embeddings_mat[9999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bef128b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_embedding_np = batch_embedding.detach().numpy()\n",
    "for i in range(16):\n",
    "    embeddings_mat[i] = batch_embedding_np[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7d671087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check\n",
      "16\n",
      "1\n",
      "check\n",
      "16\n",
      "2\n",
      "check\n",
      "16\n",
      "3\n",
      "check\n",
      "16\n",
      "4\n",
      "check\n",
      "16\n",
      "5\n",
      "check\n",
      "16\n",
      "6\n",
      "check\n",
      "16\n",
      "7\n",
      "check\n",
      "16\n",
      "8\n",
      "check\n",
      "16\n",
      "9\n",
      "check\n",
      "16\n",
      "10\n",
      "(10000, 512)\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "with torch.no_grad():\n",
    "    print('start check')\n",
    "    for batch_idx, (features,_) in enumerate(train_loader):\n",
    "        batch_embedding = model(features)\n",
    "        batch_embedding_np = batch_embedding.detach().numpy()\n",
    "        for i in range(batch_size):\n",
    "            embeddings_mat[batch_size*batch_idx+i] = batch_embedding_np[i]\n",
    "            \n",
    "        counter+=1\n",
    "        print(counter)\n",
    "        \n",
    "        #if counter == 10:\n",
    "            #break\n",
    "            \n",
    "print(embeddings_mat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "083bcc18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.34106088e+00 1.26488709e+00 4.50520217e-01 2.18269324e+00\n",
      " 2.41707039e+00 1.54707217e+00 3.96297015e-02 7.47919023e-01\n",
      " 6.58406019e-01 3.03856403e-01 1.75959134e+00 1.84254920e+00\n",
      " 2.54536420e-01 1.66963196e+00 5.80229223e-01 4.04665351e-01\n",
      " 1.93388388e-01 7.51216829e-01 5.57201505e-01 2.03636542e-01\n",
      " 1.12699592e+00 8.60119402e-01 3.60306650e-01 4.57709998e-01\n",
      " 5.33537388e-01 1.89176464e+00 8.00051451e-01 3.15936297e-01\n",
      " 1.35050511e+00 2.12067544e-01 7.52931774e-01 1.24361515e+00\n",
      " 1.05719972e+00 3.70105118e-01 7.69606948e-01 1.04001653e+00\n",
      " 1.42948699e+00 9.34208453e-01 3.83281320e-01 4.57466662e-01\n",
      " 2.36585230e-01 1.32742071e+00 1.08404601e+00 2.15726113e+00\n",
      " 1.80390632e+00 4.35263544e-01 2.01469183e-01 2.87685752e-01\n",
      " 1.55186617e+00 7.02363789e-01 9.46308553e-01 1.15775466e+00\n",
      " 8.72832015e-02 1.32500827e+00 1.71733749e+00 1.77143002e+00\n",
      " 2.44687247e+00 1.96903303e-01 5.98870397e-01 6.85298145e-01\n",
      " 8.33562016e-01 1.02963614e+00 1.37599015e+00 6.25699580e-01\n",
      " 2.57978797e+00 7.28679597e-01 4.69501972e-01 1.60842359e-01\n",
      " 3.19875097e+00 1.96423978e-01 6.74206197e-01 1.59843409e+00\n",
      " 1.59022892e+00 1.37732178e-01 3.57507728e-02 3.93939823e-01\n",
      " 3.21543008e-01 4.11894262e-01 3.81167620e-01 6.61017358e-01\n",
      " 2.77244657e-01 4.46389496e-01 1.07591772e+00 1.34074652e+00\n",
      " 7.09140956e-01 2.38720870e+00 3.40897471e-01 1.62791647e-02\n",
      " 6.29570782e-01 1.26802075e+00 5.56715608e-01 9.32786882e-01\n",
      " 5.01067936e-01 3.06740689e+00 2.62499869e-01 8.60542357e-02\n",
      " 1.48023975e+00 4.78911221e-01 5.37494183e-01 1.34797871e-01\n",
      " 9.08441365e-01 1.79357708e+00 1.71499836e+00 2.39591861e+00\n",
      " 2.71070629e-01 1.54985714e+00 3.92328382e-01 7.16138542e-01\n",
      " 8.32092047e-01 7.94143155e-02 6.78996503e-01 2.28403425e+00\n",
      " 5.99127948e-01 2.27162570e-01 4.19542730e-01 1.87188476e-01\n",
      " 1.66150606e+00 8.40302944e-01 9.04401958e-01 1.81670702e+00\n",
      " 6.32177413e-01 1.84710228e+00 7.65595555e-01 1.04517961e+00\n",
      " 8.61642003e-01 5.76071084e-01 1.23657517e-01 4.87265348e-01\n",
      " 2.40994024e+00 9.09684062e-01 1.35994244e+00 2.28932559e-01\n",
      " 1.61713624e+00 7.10817158e-01 2.04287395e-01 2.31771111e-01\n",
      " 5.10885715e-01 3.90481591e-01 7.06575811e-01 2.77477121e+00\n",
      " 3.55504900e-01 4.93965656e-01 7.13023603e-01 5.89375161e-02\n",
      " 7.77258635e-01 6.27037466e-01 1.15997481e+00 3.25937152e-01\n",
      " 7.12461770e-01 1.25625753e+00 1.15473773e-02 1.45889282e+00\n",
      " 7.24402905e-01 5.31079531e-01 6.70391381e-01 3.71115476e-01\n",
      " 4.98486280e-01 1.26785946e+00 1.63499892e+00 1.86019778e-01\n",
      " 5.84075332e-01 5.90213597e-01 1.71305990e+00 3.85506600e-01\n",
      " 6.43978417e-01 8.81175280e-01 1.53578436e+00 4.44053233e-01\n",
      " 4.10516739e-01 3.02694947e-01 1.45208931e+00 1.45893872e+00\n",
      " 1.57155886e-01 1.58732295e+00 9.09327626e-01 2.41678387e-01\n",
      " 1.42817068e+00 1.13674498e+00 1.48330176e+00 2.40023088e+00\n",
      " 5.67580640e-01 1.66830051e+00 1.78552842e+00 3.41586098e-02\n",
      " 1.75499868e+00 7.08636522e-01 3.70798588e-01 2.43889555e-01\n",
      " 1.80224621e+00 1.90978599e+00 4.27202970e-01 1.95968330e-01\n",
      " 5.23816705e-01 4.69566137e-01 1.67972624e+00 1.67150891e+00\n",
      " 1.15265238e+00 7.40515888e-01 9.61581469e-01 1.28868476e-01\n",
      " 9.06308234e-01 1.30471659e+00 7.59214699e-01 8.11661303e-01\n",
      " 1.32611585e+00 2.29664370e-01 1.83928263e+00 1.49866730e-01\n",
      " 8.08980048e-01 2.45531410e-01 1.92188472e-01 8.42949688e-01\n",
      " 8.94289196e-01 1.08752203e+00 4.83041495e-01 2.61837170e-02\n",
      " 1.12111950e+00 3.29564154e-01 9.01392460e-01 4.14133430e-01\n",
      " 8.45094740e-01 3.82398963e-01 1.03323364e+00 5.21797419e-01\n",
      " 1.73758984e+00 1.78043175e+00 1.03325033e+00 1.22637367e+00\n",
      " 8.43355596e-01 1.28604388e+00 9.09993410e-01 1.81390628e-01\n",
      " 3.26159269e-01 2.59140706e+00 5.29312622e-03 1.17379010e+00\n",
      " 1.33486474e+00 2.72550893e+00 1.01550490e-01 1.13477516e+00\n",
      " 5.43603480e-01 6.36794090e-01 1.66034353e+00 1.69036865e+00\n",
      " 5.29535770e-01 8.15773606e-01 1.02372575e+00 1.25693393e+00\n",
      " 2.22662985e-01 3.49193645e+00 3.27344924e-01 4.74730968e-01\n",
      " 4.16635990e+00 1.66045368e-01 6.47170305e-01 1.18054926e+00\n",
      " 3.00915122e-01 1.65350890e+00 1.00639164e-01 1.64674520e+00\n",
      " 1.41526282e+00 4.61210221e-01 7.98804760e-01 1.56943977e+00\n",
      " 1.73249912e+00 6.03399754e-01 6.72428668e-01 9.54715133e-01\n",
      " 2.02703118e-01 1.04340769e-01 1.00299671e-01 1.44229472e+00\n",
      " 6.71086609e-01 3.39095354e-01 5.98860443e-01 1.09366107e+00\n",
      " 3.45050842e-01 5.06042659e-01 7.45282173e-01 1.20756313e-01\n",
      " 1.17079234e+00 1.45775974e+00 7.13813826e-02 3.50058705e-01\n",
      " 1.99001551e+00 6.85853697e-03 1.91925657e+00 2.35642385e+00\n",
      " 6.85967684e-01 7.40263164e-01 3.76139045e-01 7.30107665e-01\n",
      " 2.49759987e-01 1.45166886e+00 2.67593169e+00 1.92525238e-02\n",
      " 1.15575147e+00 1.18951774e+00 1.12904334e+00 4.22193371e-02\n",
      " 1.27265990e+00 1.70368397e+00 1.93793595e-01 4.24050868e-01\n",
      " 1.93134725e+00 1.59758723e+00 1.77294940e-01 1.42861938e+00\n",
      " 1.66431025e-01 2.41404888e-03 2.51316237e+00 4.24021259e-02\n",
      " 1.83877110e+00 4.21428569e-02 4.30567205e-01 1.31411958e+00\n",
      " 1.69650173e+00 2.28177592e-01 1.37254536e+00 2.46207312e-01\n",
      " 2.10019246e-01 9.51822698e-01 9.49097991e-01 1.94020078e-01\n",
      " 9.65924799e-01 4.59902078e-01 1.44398558e+00 2.13678575e+00\n",
      " 6.85287476e-01 1.41067219e+00 1.11545539e+00 2.62915206e+00\n",
      " 7.94486761e-01 9.04237628e-01 7.74110377e-01 1.28689861e+00\n",
      " 1.72006905e+00 2.43293628e-01 1.08215785e+00 5.34236789e-01\n",
      " 8.88227820e-01 7.68097639e-01 1.13985729e+00 6.55762732e-01\n",
      " 1.25412738e+00 1.62787926e+00 2.00962767e-01 8.09218287e-01\n",
      " 6.19184911e-01 1.97857881e+00 2.99217880e-01 4.32286859e-01\n",
      " 2.77403533e-01 9.68478560e-01 2.26621294e+00 5.16077951e-02\n",
      " 1.15292478e+00 1.23026741e+00 3.09603930e-01 5.34395099e-01\n",
      " 7.97422349e-01 1.80545890e+00 9.89531636e-01 1.42774856e+00\n",
      " 5.96726298e-01 6.56437099e-01 1.39413083e+00 1.41746378e+00\n",
      " 1.65910572e-01 1.05271769e+00 1.12241840e+00 4.84753430e-01\n",
      " 2.00016761e+00 1.03830373e+00 6.58934116e-01 2.73708552e-01\n",
      " 1.06981732e-01 5.21811724e-01 2.26503611e-01 1.62423506e-01\n",
      " 2.03686857e+00 4.23433423e-01 5.54491758e-01 2.18071246e+00\n",
      " 2.02560806e+00 3.73056501e-01 1.05460620e+00 2.98674345e-01\n",
      " 2.78785944e-01 3.56152982e-01 1.20141721e+00 5.04172325e-01\n",
      " 2.85363108e-01 1.20089912e+00 1.03008533e+00 3.30691263e-02\n",
      " 1.18186128e+00 8.40352893e-01 1.64954090e+00 7.48213157e-02\n",
      " 8.15839469e-01 4.08061177e-01 7.61361793e-02 3.50743473e-01\n",
      " 1.56314373e+00 1.17751944e+00 1.10597336e+00 1.10369265e+00\n",
      " 3.50977302e-01 3.57517630e-01 6.49106860e-01 6.52962744e-01\n",
      " 2.03344151e-01 9.35979187e-01 5.46294212e-01 9.83515143e-01\n",
      " 5.83998442e-01 6.05451107e-01 4.76525545e-01 6.49906754e-01\n",
      " 3.21755260e-01 5.30759096e-01 1.43244159e+00 1.30135262e+00\n",
      " 6.07503653e-01 4.86590683e-01 1.37445450e-01 9.27253902e-01\n",
      " 5.65195501e-01 6.97689772e-01 2.81168781e-02 1.42004085e+00\n",
      " 5.49534261e-01 5.10354161e-01 9.55347642e-02 9.87408102e-01\n",
      " 1.27306134e-01 6.55479729e-01 7.25694776e-01 7.63229847e-01\n",
      " 1.27198875e+00 6.29295528e-01 1.18260252e+00 1.65439934e-01\n",
      " 1.40885568e+00 7.42162883e-01 7.36149848e-01 1.14791298e+00\n",
      " 1.31927693e+00 1.56713152e+00 4.11272913e-01 8.31845939e-01\n",
      " 1.09117322e-01 3.14303255e+00 4.69514340e-01 1.00198436e+00\n",
      " 1.15490305e+00 1.53185320e+00 2.22671437e+00 4.18440342e-01\n",
      " 5.18840373e-01 1.05894911e+00 1.19076645e+00 1.98551774e-01\n",
      " 1.37047797e-01 1.81794441e+00 2.01444840e+00 8.69655013e-01\n",
      " 3.91569100e-02 8.32334578e-01 1.84166580e-01 1.26992500e+00\n",
      " 6.15913212e-01 2.52276719e-01 1.37858975e+00 3.26427668e-01\n",
      " 1.84729850e+00 1.14602578e+00 1.69498384e+00 1.34185001e-01\n",
      " 9.64967757e-02 1.74814200e+00 1.24625838e+00 1.38235641e+00\n",
      " 1.52440846e+00 7.06434906e-01 9.29753780e-01 1.50975752e+00\n",
      " 2.86563396e+00 9.48215663e-01 2.62034941e+00 6.68009102e-01\n",
      " 2.45134562e-01 5.93149662e-01 4.57193494e-01 5.39006114e-01\n",
      " 2.31775865e-01 9.81936634e-01 2.11656308e+00 1.10474503e+00\n",
      " 3.21834475e-01 1.89057267e+00 2.00286937e+00 1.24068403e+00\n",
      " 2.51870483e-01 7.67156959e-01 1.88100010e-01 4.56419766e-01\n",
      " 1.08694172e+00 2.97594488e-01 1.91814876e+00 1.83079886e+00]\n"
     ]
    }
   ],
   "source": [
    "numpy.savetxt(\"embedding.csv\", embeddings_mat, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ed69fb7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageClassification(\n",
      "    crop_size=[224]\n",
      "    resize_size=[256]\n",
      "    mean=[0.485, 0.456, 0.406]\n",
      "    std=[0.229, 0.224, 0.225]\n",
      "    interpolation=InterpolationMode.BILINEAR\n",
      ")\n",
      "start check\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "(10000, 512)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Transform, resize and normalize the images and then use a pretrained model to extract \n",
    "the embeddings.\n",
    "\"\"\"\n",
    "\n",
    "import torchvision\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "classification_models = torchvision.models.list_models(module=torchvision.models)\n",
    "\n",
    "# Initialize model\n",
    "resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "\n",
    "train_transforms = transforms.Compose([transforms.ToTensor()])\n",
    "weights = ResNet18_Weights.DEFAULT\n",
    "print(weights.transforms())\n",
    "\n",
    "# TODO: define a transform to pre-process the images\n",
    "train_transforms = transforms.Compose([transforms.ToTensor(), weights.transforms()])\n",
    "\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root=\"dataset/\", transform=train_transforms)\n",
    "# Hint: adjust batch_size and num_workers to your PC configuration, so that you don't \n",
    "# run out of memory\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=16,\n",
    "                          shuffle=False,\n",
    "                          pin_memory=True, num_workers=4)\n",
    "\n",
    "# TODO: define a model for extraction of the embeddings (Hint: load a pretrained model,\n",
    "#  more info here: https://pytorch.org/vision/stable/models.html)\n",
    "\n",
    "\n",
    "model = torchvision.models.resnet18(pretrained = True)\n",
    "model.eval()\n",
    "\n",
    "# choose average pooling layer as feature layer\n",
    "layer = model._modules.get('avgpool')\n",
    "# ste last layer to the identity layer\n",
    "model.fc = torch.nn.Identity()\n",
    "\n",
    "batch_size=16\n",
    "samples=10000\n",
    "features=512\n",
    "\n",
    "embeddings_mat = np.zeros((samples, features))\n",
    "# TODO: Use the model to extract the embeddings. Hint: remove the last layers of the \n",
    "# model to access the embeddings the model generates. \n",
    "\n",
    "counter = 0\n",
    "with torch.no_grad():\n",
    "    print('start check')\n",
    "    for batch_idx, (features,_) in enumerate(train_loader):\n",
    "        batch_embedding = model(features)\n",
    "        batch_embedding_np = batch_embedding.detach().numpy()\n",
    "        for i in range(batch_size):\n",
    "            embeddings_mat[batch_size*batch_idx+i] = batch_embedding_np[i]\n",
    "            \n",
    "        print(counter)\n",
    "        \n",
    "        counter+=1\n",
    "        \n",
    "        \n",
    "        #if counter == 10:\n",
    "            #break\n",
    "            \n",
    "print(embeddings_mat.shape)\n",
    "\n",
    "np.save('dataset/embeddings.npy', embeddings_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a612315f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('dataset/embeddings.npy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:IML]",
   "language": "python",
   "name": "conda-env-IML-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
